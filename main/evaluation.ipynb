{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69ebef85",
   "metadata": {},
   "source": [
    "The following code is responsible for the tolerance function and generating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743b0dbf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "import unicodedata\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "# ======================================================\n",
    "# TOLERANCE FUNCTIONS (adaptadas)\n",
    "# ======================================================\n",
    "\n",
    "def normalize_tokens(s):\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = str(s).lower()\n",
    "    s = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "    s = re.sub(r'[^a-z0-9]+', ' ', s)\n",
    "    return s.split()\n",
    "\n",
    "def overlap_coefficient(s1, s2):\n",
    "    t1 = set(normalize_tokens(s1))\n",
    "    t2 = set(normalize_tokens(s2))\n",
    "\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "\n",
    "    return len(t1 & t2) / min(len(t1), len(t2))\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return 6371 * c  # km\n",
    "\n",
    "def relative_similarity(x, y):\n",
    "    try:\n",
    "        x, y = float(x), float(y)\n",
    "        return 1 - abs(x - y) / max(abs(x), abs(y), 1e-9)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def considerTolerance(correct, predicted, prop):\n",
    "    \"\"\"\n",
    "    Returns True if prediction is correct under tolerance\n",
    "    \"\"\"\n",
    "    if prop not in correct or prop not in predicted:\n",
    "        return None  # not comparable\n",
    "\n",
    "    ref = correct[prop]\n",
    "    val = predicted[prop]\n",
    "\n",
    "    if ref is None or val is None:\n",
    "        return None\n",
    "\n",
    "    # Latitude / Longitude handled together\n",
    "    if prop in [\"latitude\", \"longitude\"]:\n",
    "        if not all(k in correct and k in predicted for k in [\"latitude\", \"longitude\"]):\n",
    "            return None\n",
    "\n",
    "        dist = haversine(\n",
    "            float(predicted[\"latitude\"]),\n",
    "            float(predicted[\"longitude\"]),\n",
    "            float(correct[\"latitude\"]),\n",
    "            float(correct[\"longitude\"])\n",
    "        )\n",
    "        return dist < 10  # km tolerance\n",
    "\n",
    "    # Strings\n",
    "    if isinstance(val, str):\n",
    "        sim = overlap_coefficient(ref, val)\n",
    "        return sim >= 0.75\n",
    "\n",
    "    # Numbers\n",
    "    try:\n",
    "        sim = relative_similarity(val, ref)\n",
    "        return sim >= 0.75\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ======================================================\n",
    "\n",
    "def getEntityDiff(entity):\n",
    "    for ent in resultsTasks:\n",
    "        icao = ent[0]\n",
    "        if icao == entity:\n",
    "            return ent[3]['entityDiff:']\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "def evaluate_results(\n",
    "    results_csv,\n",
    "    ground_truth_csv,\n",
    "    id_column=\"icao\"\n",
    "):\n",
    "    \"\"\"\n",
    "    results_csv: CSV from majority vote (one row per entity)\n",
    "    ground_truth_csv: CSV with GT\n",
    "    id_column: entity identifier (present in GT)\n",
    "    \"\"\"\n",
    "\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    \n",
    "    gt_df = pd.read_csv(ground_truth_csv)\n",
    "    # Index by id\n",
    "    gt_df = gt_df.set_index(id_column)\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    detailed_rows = []\n",
    "\n",
    "    for index, pred_row in results_df.iterrows():\n",
    "        \n",
    "        \n",
    "\n",
    "        entity_id = pred_row['test'][:4]\n",
    "        prompt_id = pred_row['test'].split(\"_\")[1]\n",
    "        diff_id = pred_row['test'].split(\"_\")[2]\n",
    "        diff_entity_id = getEntityDiff(entity_id)\n",
    "    \n",
    "\n",
    "        if entity_id not in gt_df.index:\n",
    "            continue\n",
    "\n",
    "        gt_row = gt_df.loc[entity_id]\n",
    "\n",
    "        predicted = pred_row.dropna().to_dict()\n",
    "        correct = gt_row.dropna().to_dict()\n",
    "\n",
    "        for prop in predicted:\n",
    "            if prop == id_column:\n",
    "                continue\n",
    "            if prop not in correct:\n",
    "                continue  # do not penalize missing GT or missing result\n",
    "\n",
    "            tol = considerTolerance(correct, predicted, prop)\n",
    "            if tol is None:\n",
    "                continue\n",
    "\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1 if tol else 0)\n",
    "\n",
    "            detailed_rows.append({\n",
    "                \"id\": entity_id,\n",
    "                \"test\": pred_row['test'],\n",
    "                \"prompt\": prompt_id,\n",
    "                \"diff\":diff_id,\n",
    "                \"diff_ent\":diff_entity_id,\n",
    "                \"property\": prop,\n",
    "                \"predicted\": predicted[prop],\n",
    "                \"ground_truth\": correct[prop],\n",
    "                \"correct\": tol\n",
    "            })\n",
    "\n",
    "    # Metrics\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred) if y_true else None,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0) if y_true else None,\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0) if y_true else None,\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0) if y_true else None,\n",
    "        \"total_comparisons\": len(y_true)\n",
    "    }\n",
    "\n",
    "   \n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_rows)\n",
    "    \n",
    "     # =====================\n",
    "    # Metrics per prompt\n",
    "    # =====================\n",
    "    prompt_metrics = {}\n",
    "\n",
    "    for prompt, group in detailed_df.groupby(\"prompt\"):\n",
    "        y_true_p = [1] * len(group)\n",
    "        y_pred_p = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "        prompt_metrics[prompt] = {\n",
    "            \"accuracy\": accuracy_score(y_true_p, y_pred_p) if y_true_p else None,\n",
    "            \"precision\": precision_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"recall\": recall_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"f1\": f1_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"total_comparisons\": len(y_true_p)\n",
    "        }\n",
    "\n",
    "    diff_metric = {}\n",
    "\n",
    "    for diff, group in detailed_df.groupby(\"diff_ent\"):\n",
    "        y_true_p = [1] * len(group)\n",
    "        y_pred_p = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "        diff_metric[diff] = {\n",
    "            \"accuracy\": accuracy_score(y_true_p, y_pred_p) if y_true_p else None,\n",
    "            \"precision\": precision_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"recall\": recall_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"f1\": f1_score(y_true_p, y_pred_p, zero_division=0),\n",
    "            \"total_comparisons\": len(y_true_p)\n",
    "        }\n",
    "\n",
    "    return metrics, prompt_metrics, detailed_df, diff_metric\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# RUN\n",
    "# ======================================================\n",
    "\n",
    "global_metrics, prompt_metrics, detailed, diff_metric = evaluate_results(\n",
    "    results_csv=\"gpt5_airports_majority_vote.csv\",\n",
    "    ground_truth_csv=\"ground_truth.csv\",\n",
    ")\n",
    "\n",
    "print(\"=== Global metrics ===\")\n",
    "for k, v in global_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Metrics per prompt ===\")\n",
    "for prompt, metrics in prompt_metrics.items():\n",
    "    print(f\"\\nPrompt {prompt}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n=== Metrics per prompt ===\")\n",
    "for diff, metrics in diff_metric.items():\n",
    "    print(f\"\\ndiff {diff}\")\n",
    "    for k, v in metrics.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "detailed.to_csv(\"evaluation_detailed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8787d29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ======================================================\n",
    "# ALL-IN-ONE CELL: Evaluation + Difficulty vs F1 Scatter\n",
    "# ======================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "\n",
    "# ======================================================\n",
    "# TOLERANCE FUNCTIONS\n",
    "# ======================================================\n",
    "\n",
    "def normalize_tokens(s):\n",
    "    if s is None:\n",
    "        return []\n",
    "    s = str(s).lower()\n",
    "    s = ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "    s = re.sub(r'[^a-z0-9]+', ' ', s)\n",
    "    return s.split()\n",
    "\n",
    "def overlap_coefficient(s1, s2):\n",
    "    t1 = set(normalize_tokens(s1))\n",
    "    t2 = set(normalize_tokens(s2))\n",
    "    if not t1 or not t2:\n",
    "        return 0.0\n",
    "    return len(t1 & t2) / min(len(t1), len(t2))\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return 6371 * c  # km\n",
    "\n",
    "def relative_similarity(x, y):\n",
    "    try:\n",
    "        x, y = float(x), float(y)\n",
    "        return 1 - abs(x - y) / max(abs(x), abs(y), 1e-9)\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def considerTolerance(correct, predicted, prop):\n",
    "    if prop not in correct or prop not in predicted:\n",
    "        return None\n",
    "\n",
    "    ref = correct[prop]\n",
    "    val = predicted[prop]\n",
    "\n",
    "    if ref is None or val is None:\n",
    "        return None\n",
    "\n",
    "    if prop in [\"latitude\", \"longitude\"]:\n",
    "        if not all(k in correct and k in predicted for k in [\"latitude\", \"longitude\"]):\n",
    "            return None\n",
    "        dist = haversine(\n",
    "            float(predicted[\"latitude\"]),\n",
    "            float(predicted[\"longitude\"]),\n",
    "            float(correct[\"latitude\"]),\n",
    "            float(correct[\"longitude\"])\n",
    "        )\n",
    "        return dist < 10\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        return overlap_coefficient(ref, val) >= 0.75\n",
    "\n",
    "    try:\n",
    "        return relative_similarity(val, ref) >= 0.75\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# ======================================================\n",
    "# LOAD DIFFICULTY CSV\n",
    "# ======================================================\n",
    "\n",
    "def load_difficulty_csv(path):\n",
    "    df = pd.read_csv(path, sep=\";\")\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    for col in [\"SpecH\", \"SpecG\", \"diff\"]:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.replace(\",\", \".\", regex=False)\n",
    "            .astype(float)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "# ======================================================\n",
    "# EVALUATION FUNCTION\n",
    "# ======================================================\n",
    "\n",
    "def evaluate_results(results_csv, ground_truth_csv, id_column=\"icao\"):\n",
    "\n",
    "    results_df = pd.read_csv(results_csv)\n",
    "    gt_df = pd.read_csv(ground_truth_csv).set_index(id_column)\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    detailed_rows = []\n",
    "\n",
    "    for _, pred_row in results_df.iterrows():\n",
    "\n",
    "        entity_id = pred_row[\"test\"][:4]\n",
    "        prompt_id = pred_row[\"test\"].split(\"_\")[1]\n",
    "        diff_id = pred_row[\"test\"].split(\"_\")[2]\n",
    "\n",
    "        if entity_id not in gt_df.index:\n",
    "            continue\n",
    "\n",
    "        predicted = pred_row.dropna().to_dict()\n",
    "        correct = gt_df.loc[entity_id].dropna().to_dict()\n",
    "\n",
    "        for prop in predicted:\n",
    "            if prop == id_column or prop not in correct:\n",
    "                continue\n",
    "\n",
    "            tol = considerTolerance(correct, predicted, prop)\n",
    "            if tol is None:\n",
    "                continue\n",
    "\n",
    "            y_true.append(1)\n",
    "            y_pred.append(1 if tol else 0)\n",
    "\n",
    "            detailed_rows.append({\n",
    "                \"id\": entity_id,\n",
    "                \"prompt\": prompt_id,\n",
    "                \"diff\": diff_id,\n",
    "                \"property\": prop,\n",
    "                \"correct\": tol\n",
    "            })\n",
    "\n",
    "    detailed_df = pd.DataFrame(detailed_rows)\n",
    "\n",
    "    global_metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"total_comparisons\": len(y_true)\n",
    "    }\n",
    "\n",
    "    # =====================\n",
    "    # Metrics per entity\n",
    "    # =====================\n",
    "    entity_metrics = {}\n",
    "\n",
    "    for entity, group in detailed_df.groupby(\"id\"):\n",
    "        y_true_e = [1] * len(group)\n",
    "        y_pred_e = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "        entity_metrics[entity] = {\n",
    "            \"f1\": f1_score(y_true_e, y_pred_e, zero_division=0),\n",
    "            \"accuracy\": accuracy_score(y_true_e, y_pred_e),\n",
    "            \"total_comparisons\": len(y_true_e)\n",
    "        }\n",
    "\n",
    "    entity_metrics_df = (\n",
    "        pd.DataFrame.from_dict(entity_metrics, orient=\"index\")\n",
    "          .reset_index()\n",
    "          .rename(columns={\"index\": \"Entity\"})\n",
    "    )\n",
    "\n",
    "    return global_metrics, detailed_df, entity_metrics_df\n",
    "\n",
    "# ======================================================\n",
    "# RUN\n",
    "# ======================================================\n",
    "\n",
    "global_metrics, detailed_df, entity_metrics_df = evaluate_results(\n",
    "    results_csv=\"claude_sonnet_airports_majority_vote.csv\",\n",
    "    ground_truth_csv=\"ground_truth.csv\"\n",
    ")\n",
    "\n",
    "print(\"=== Global metrics ===\")\n",
    "for k, v in global_metrics.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# ======================================================\n",
    "# DIFFICULTY vs F1 SCATTER\n",
    "# ======================================================\n",
    "\n",
    "difficulty_df = load_difficulty_csv(\"diff_per_task.csv\")\n",
    "\n",
    "# Average difficulty per entity\n",
    "difficulty_entity = (\n",
    "    difficulty_df.groupby(\"Entity\", as_index=False)[\"diff\"]\n",
    "    .mean()\n",
    ")\n",
    "\n",
    "entity_diff_metrics = []\n",
    "\n",
    "for (entity, diff), group in detailed_df.groupby([\"id\", \"diff\"]):\n",
    "    y_true_ed = [1] * len(group)\n",
    "    y_pred_ed = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "    entity_diff_metrics.append({\n",
    "        \"Entity\": entity,\n",
    "        \"diff\": diff,\n",
    "        \"f1\": f1_score(y_true_ed, y_pred_ed, zero_division=0),\n",
    "        \"accuracy\": accuracy_score(y_true_ed, y_pred_ed),\n",
    "        \"total_comparisons\": len(y_true_ed)\n",
    "    })\n",
    "# Load numeric difficulty once\n",
    "diff_df = pd.read_csv(\"diff_per_task.csv\", sep=\";\")\n",
    "\n",
    "# Normalize keys\n",
    "diff_df[\"Entity\"] = diff_df[\"Entity\"].astype(str).str.strip().str.upper()\n",
    "diff_df[\"diffProps\"] = diff_df[\"diffProps\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Parse numeric diff\n",
    "diff_df[\"diff_numeric\"] = (\n",
    "    diff_df[\"diff\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\",\", \".\", regex=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Build a lookup dict: (Entity, diff) -> diff_numeric\n",
    "diff_lookup = {\n",
    "    (row[\"Entity\"], row[\"diffProps\"]): row[\"diff_numeric\"]\n",
    "    for _, row in diff_df.iterrows()\n",
    "}\n",
    "\n",
    "# ======================================================\n",
    "# F1 per (Entity, diff) + numeric diff\n",
    "# ======================================================\n",
    "\n",
    "entity_diff_metrics = []\n",
    "\n",
    "for (entity, diff), group in detailed_df.groupby([\"id\", \"diff\"]):\n",
    "    y_true_ed = [1] * len(group)\n",
    "    y_pred_ed = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "    entity_diff_metrics.append({\n",
    "        \"Entity\": entity,\n",
    "        \"diff\": diff,\n",
    "        \"f1\": f1_score(y_true_ed, y_pred_ed, zero_division=0),\n",
    "       \n",
    "        \n",
    "        \"diff_numeric\": diff_lookup.get(\n",
    "            (entity.strip().upper(), diff.strip().lower()),\n",
    "            np.nan\n",
    "        )\n",
    "    })\n",
    "\n",
    "entity_diff_metrics_df = pd.DataFrame(entity_diff_metrics)\n",
    "\n",
    "entity_diff_metrics_df = pd.DataFrame(entity_diff_metrics)\n",
    "entity_diff_metrics_df.to_csv('entity_diff_f1.csv',index=False, sep=';', decimal=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9bb50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "entity_prompt_diff_metrics = []\n",
    "\n",
    "for (entity, prompt, diff_props), group in detailed_df.groupby(\n",
    "    [\"id\", \"prompt\", \"diff\"]\n",
    "):\n",
    "    y_true = [1] * len(group)\n",
    "    y_pred = group[\"correct\"].astype(int).tolist()\n",
    "\n",
    "    entity_prompt_diff_metrics.append({\n",
    "        \"entity\": entity,\n",
    "        \"prompt\": prompt,\n",
    "        \"diffProps\": diff_props,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred) if y_true else None,\n",
    "        \"precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"f1\": f1_score(y_true, y_pred, zero_division=0),\n",
    "    })\n",
    "\n",
    "entity_prompt_diff_df = pd.DataFrame(entity_prompt_diff_metrics)\n",
    "entity_prompt_diff_dfNew = entity_prompt_diff_df.rename(\n",
    "    columns={\"diffProps\": \"diffPropQuartile\"}\n",
    ")\n",
    "\n",
    "def map_prompt_name(prompt_id: str) -> str:\n",
    "    mapping = {\n",
    "        \"p0\": \"compact\",\n",
    "        \"p1\": \"few5\",\n",
    "        \"p2\": \"few3\",\n",
    "        \"p3\": \"few1\",\n",
    "        \"p4\": \"zero\",\n",
    "        \"p5\": \"5cot\",\n",
    "        \"p6\": \"zerocot\",\n",
    "        \"p7\": \"3cot\",\n",
    "        \"p8\": \"1cot\",\n",
    "    }\n",
    "\n",
    "    if prompt_id not in mapping:\n",
    "        raise ValueError(f\"Unknown prompt id: {prompt_id}\")\n",
    "\n",
    "    return mapping[prompt_id]\n",
    "\n",
    "entity_prompt_diff_dfNew[\"prompt\"] = (\n",
    "    entity_prompt_diff_dfNew[\"prompt\"].apply(map_prompt_name)\n",
    ")\n",
    "\n",
    "entity_prompt_diff_dfNew"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
